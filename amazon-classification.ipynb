{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ntf.test.is_gpu_available()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:21.737621Z","iopub.execute_input":"2022-05-18T03:34:21.738313Z","iopub.status.idle":"2022-05-18T03:34:21.751980Z","shell.execute_reply.started":"2022-05-18T03:34:21.738273Z","shell.execute_reply":"2022-05-18T03:34:21.751169Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## Def functions: One-Hot Encoding and Split Data (Dataframe)","metadata":{}},{"cell_type":"code","source":"def Get_ListOfLabels(df_InputLabels):\n    df_Tags = df_InputLabels.str.split(expand=True)\n    columns = len(df_Tags.columns)\n\n    for i in range(columns):\n        if i == 0:\n            df_aux = pd.DataFrame(df_Tags[0])\n        else:\n            df_aux = df_aux.append(pd.DataFrame(df_Tags[i].rename(index=0)))\n\n    LabelList = (df_aux.drop_duplicates().sort_values([0], ascending=[True]))[0].tolist()\n    LabelList = list(filter(None, LabelList))\n    return LabelList\n\n\ndef Get_LabelEncodedArray(df_InputLabels, LabelList):\n    LabelArray = np.zeros((df_InputLabels.count(), len(LabelList)))\n    TagsArray = df_InputLabels.str.split(expand=True).to_numpy()\n\n    for i in range(TagsArray.shape[0]):\n        for j in range(TagsArray.shape[1]):\n            for k in range(len(LabelList)):\n                if(TagsArray[i][j]== LabelList[k]):\n                    LabelArray[i][k] = 1\n\n    return LabelArray\n\n\ndef SplitData(df_Dataset, Ratio):\n\n    df_TrainData = pd.DataFrame(df_Dataset).sample(round(Ratio * df_Dataset['filename'].count()))\n\n    df_ValData = pd.DataFrame(df_Dataset)\n    condition = df_ValData['filename'].isin(df_TrainData['filename'])\n    df_ValData = df_ValData.drop(df_ValData[condition].index)\n\n    return df_TrainData, df_ValData","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:21.892673Z","iopub.execute_input":"2022-05-18T03:34:21.893132Z","iopub.status.idle":"2022-05-18T03:34:21.904181Z","shell.execute_reply.started":"2022-05-18T03:34:21.893096Z","shell.execute_reply":"2022-05-18T03:34:21.903079Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"- Coding labels -> One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"source_path = \"../input/amazon-satellite/raw-data\"\nsource_path_Labels = os.path.join(source_path, 'train_v2.csv')\nsource_path_Images = os.path.join(source_path, 'train-jpg')\n\ndf_Raw = pd.read_csv(source_path_Labels)\nLabelList = Get_ListOfLabels(df_Raw.tags)\n\nGet_LabelEncodedArray(df_Raw.tags, LabelList)\ndf_Dataset = pd.DataFrame(Get_LabelEncodedArray(df_Raw.tags, LabelList), columns=LabelList)\ndf_Dataset['filename'] = df_Raw.image_name.to_list()\ndf_Dataset['filename'] = df_Dataset['filename']+'.jpg'\ndf_Dataset = df_Dataset.loc[: , ['filename']+LabelList]","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:22.035865Z","iopub.execute_input":"2022-05-18T03:34:22.036065Z","iopub.status.idle":"2022-05-18T03:34:27.249491Z","shell.execute_reply.started":"2022-05-18T03:34:22.036041Z","shell.execute_reply":"2022-05-18T03:34:27.248731Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Training and Validation Generators\n\nNow that you know the images you are dealing with, it is time for you to code the generators that will fed these images to your Network. For this, complete the `train_val_generators` function below:\n\n**Important Note:** The images have a resolution of 300x300 but the `flow_from_directory` method you will use allows you to set a target resolution. In this case, **set a `target_size` of (150, 150)**. This will heavily lower the number of trainable parameters in your final network, yielding much quicker training times without compromising the accuracy!","metadata":{}},{"cell_type":"code","source":"# GRADED FUNCTION: train_val_generators\ndef train_val_generators(DIRECTORY, df_Train, df_Validation):\n  ### START CODE HERE\n\n  # Instantiate the ImageDataGenerator class \n  # Don't forget to normalize pixel values and set arguments to augment the images \n  train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n                                     rotation_range=10,\n                                     width_shift_range=0.1,\n                                     height_shift_range=0.1,\n                                     horizontal_flip=True,\n                                     fill_mode='nearest')\n\n  # Pass in the appropriate arguments to the flow_from_dataframe method\n  train_generator = train_datagen.flow_from_dataframe(dataframe=df_Train,\n                                                      directory=DIRECTORY,\n                                                      x_col=df_Train.columns[0],\n                                                      y_col=df_Train.columns[1:len(df_Train.columns)],\n                                                      batch_size=32, \n                                                      class_mode='raw',\n                                                      target_size=(256, 256))\n\n  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n  # Remember that validation data should not be augmented\n  validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\n  # Pass in the appropriate arguments to the flow_from_dataframe method\n  validation_generator = validation_datagen.flow_from_dataframe(dataframe=df_Validation,\n                                                                directory=DIRECTORY,\n                                                                x_col=df_ValData.columns[0],\n                                                                y_col=df_ValData.columns[1:len(df_ValData.columns)],\n                                                                batch_size=32, \n                                                                class_mode='raw',\n                                                                target_size=(256, 256))\n  ### END CODE HERE\n  return train_generator, validation_generator","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:27.251042Z","iopub.execute_input":"2022-05-18T03:34:27.251298Z","iopub.status.idle":"2022-05-18T03:34:27.263213Z","shell.execute_reply.started":"2022-05-18T03:34:27.251266Z","shell.execute_reply":"2022-05-18T03:34:27.262380Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Split Data\nTrainToValidation_DataSize_Ratio = 0.8\ndf_TrainData, df_ValData = SplitData(df_Dataset, TrainToValidation_DataSize_Ratio)\n\n# Test Generators\ntrain_generator, validation_generator = train_val_generators(source_path_Images, df_TrainData, df_ValData)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:27.265674Z","iopub.execute_input":"2022-05-18T03:34:27.266168Z","iopub.status.idle":"2022-05-18T03:34:41.076617Z","shell.execute_reply.started":"2022-05-18T03:34:27.266035Z","shell.execute_reply":"2022-05-18T03:34:41.075845Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Model Definition","metadata":{}},{"cell_type":"markdown","source":"- Download InceptionV3 weights","metadata":{}},{"cell_type":"code","source":"!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:41.078724Z","iopub.execute_input":"2022-05-18T03:34:41.078967Z","iopub.status.idle":"2022-05-18T03:34:42.497477Z","shell.execute_reply.started":"2022-05-18T03:34:41.078935Z","shell.execute_reply":"2022-05-18T03:34:42.496624Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.optimizers import RMSprop\nimport tensorflow_addons as tfa\n\ndef create_model(NumOfClasses):\n    \n    #Load InceptionV3 Model\n    local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    InceptionModel = InceptionV3(input_shape = (256, 256, 3), include_top = False, weights = local_weights_file)\n    #Make all layers on InceptionV3 untrainable\n    for layer in InceptionModel.layers:\n        layer.trainable = False\n    #Get Last Convolution Layer\n    InceptionDesiredLayer = InceptionModel.get_layer('mixed10').output\n\n    #Define Top Layers connected to InceptionV3\n    x = tf.keras.layers.Flatten()(InceptionDesiredLayer)\n    x = tf.keras.layers.Dense(2048, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(256, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(NumOfClasses, activation='sigmoid')(x)\n\n    model = tf.keras.Model(inputs=InceptionModel.input, outputs=x)\n\n    model.compile(optimizer=RMSprop(learning_rate=0.0005),\n                loss='binary_crossentropy',\n                metrics=['Precision',\n                         'Recall', \n                         tfa.metrics.FBetaScore(beta=2.0, num_classes=NumOfClasses, average='micro', threshold=0.1),\n                         tfa.metrics.F1Score(num_classes=NumOfClasses, average='micro', threshold=0.1), 'CategoricalAccuracy'\n                        ]\n               ) \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:42.499324Z","iopub.execute_input":"2022-05-18T03:34:42.499608Z","iopub.status.idle":"2022-05-18T03:34:42.517064Z","shell.execute_reply.started":"2022-05-18T03:34:42.499570Z","shell.execute_reply":"2022-05-18T03:34:42.516293Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('val_fbeta_score')>0.90):\n      print(\"\\nReached FBeta Score = 0.9. Stop Training!\")\n      self.model.stop_training = True","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:42.519898Z","iopub.execute_input":"2022-05-18T03:34:42.520101Z","iopub.status.idle":"2022-05-18T03:34:42.666740Z","shell.execute_reply.started":"2022-05-18T03:34:42.520076Z","shell.execute_reply":"2022-05-18T03:34:42.665621Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# Get the untrained model\nmodel = create_model(len(df_Dataset.columns)-1)\n\n# Train the model\nhistory = model.fit(train_generator,\n                    epochs=40,\n                    verbose=1,\n                    validation_data=validation_generator,\n                    callbacks=myCallback()\n                   )","metadata":{"execution":{"iopub.status.busy":"2022-05-18T03:34:50.539319Z","iopub.execute_input":"2022-05-18T03:34:50.539952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot History","metadata":{}},{"cell_type":"code","source":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nfbeta_score = history.history['fbeta_score']\nval_fbeta_score = history.history['val_fbeta_score']\n\nf1=history.history['f1_score']\nval_f1 = history.history['val_f1_score']\n\nrecall = history.history['recall']\nval_recall = history.history['val_recall']\n\nprecision = history.history['precision']\nval_precision = history.history['val_precision']\n\nacc=history.history['categorical_accuracy']\nval_acc=history.history['val_categorical_accuracy']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, label=\"Training Loss\")\nplt.plot(epochs, val_loss, label=\"Validation Loss\")\nplt.title('Training and validation Loss')\nplt.legend()\nplt.show()\n\n#------------------------------------------------\n# Plot training and validation F1-Score per epoch\n#------------------------------------------------\nplt.plot(epochs, f1, label=\"F-Beta-Score\")\nplt.plot(epochs, val_f1, label=\"Validation F-Beta-Score\")\nplt.title('Training and validation F-Beta-Score')\nplt.legend()\nplt.show()\n\n#------------------------------------------------\n# Plot training and validation F-Beta-Score per epoch\n#------------------------------------------------\nplt.plot(epochs, fbeta_score, label=\"F1-Score\")\nplt.plot(epochs, val_fbeta_score, label=\"Validation F1-Score\")\nplt.title('Training and validation F1-Score')\nplt.legend()\nplt.show()\n\n#------------------------------------------------\n# Plot training and validation Precision per epoch\n#------------------------------------------------\nplt.plot(epochs, precision, label=\"Precision\")\nplt.plot(epochs, val_precision, label=\"Validation Precision\")\nplt.title('Training and validation Precision')\nplt.legend()\nplt.show()\n\n#------------------------------------------------\n# Plot training and validation Recall per epoch\n#------------------------------------------------\nplt.plot(epochs, recall, label=\"Recall\")\nplt.plot(epochs, val_recall, label=\"Validation Recall\")\nplt.title('Training and validation Recall')\nplt.legend()\nplt.show()\n\n#------------------------------------------------\n# Plot training and validation Recall per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, label=\"Accuracy\")\nplt.plot(epochs, val_acc, label=\"Validation Accuracy\")\nplt.title('Training and validation Categorical Accuracy')\nplt.legend()\nplt.show()\n\n\nprint(\"\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T01:16:12.212793Z","iopub.execute_input":"2022-05-18T01:16:12.213064Z","iopub.status.idle":"2022-05-18T01:16:12.239283Z","shell.execute_reply.started":"2022-05-18T01:16:12.213013Z","shell.execute_reply":"2022-05-18T01:16:12.238377Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Save History to Pickle file","metadata":{}},{"cell_type":"code","source":"def download_history():\n  import pickle\n\n  with open('history.pkl', 'wb') as f:\n    pickle.dump(history.history, f)\n\ndownload_history()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}